{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist datasetæ˜¯æ‰‹å†™æ•°å­—çš„è®­ç»ƒæ•°æ®ï¼Œé¡¹ç›®çš„æœ¬è´¨ä¸Šæ˜¯è§£é‡Šå¦‚ä½•å°†å›¾ç‰‡åƒç´ æ•°æ®è½¬æ¢æˆå¯ä»¥åœ¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸­ä½¿ç”¨çš„æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 784)\n",
      "Y shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "xs, ys = mnist.train.next_batch(batch_size)\n",
    "print(\"X shape:\",xs.shape)\n",
    "print(\"Y shape:\",ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ·±åº¦å­¦ä¹ ä¸€æ–¹é¢éœ€è¦æ¿€æ´»å‡½æ•°å®ç°éçº¿æ€§åŒ–ï¼Œ\n",
    "### å¦å¤–ä¸€æ–¹é¢éœ€è¦ä½¿ç”¨ä¸€ä¸ªæˆ–è€…å¤šä¸ªéšè—å±‚ä½¿å¾—ç¥ç»ç½‘ç»œçš„ç»“æ„æ›´æ·±ï¼Œä»¥è§£å†³å¤æ‚é—®é¢˜\n",
    "\n",
    "> ä½†æ˜¯éšç€ç¥ç»ç½‘ç»œçš„ç»“æ„å˜å¾—å¤æ‚ï¼Œéœ€è¦å¸¦æŒ‡æ•°è¡°å‡çš„å­¦ä¹ ç‡è®¾ç½®ï¼Œä»¥ä¿è¯æ¢¯åº¦ä¸‹é™å®¹æ˜“æ”¶æ•›ï¼Œä½¿ç”¨æ­£åˆ™åŒ–æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨æ»‘åŠ¨å¹³å‡\n",
    "> æ¨¡å‹æ¥ä½¿å¾—æœ€ç»ˆçš„æ¨¡å‹æ›´åŠ å¥å£®\n",
    "> æ»‘åŠ¨å¹³å‡ğ‘¡ æ—¶åˆ»å˜é‡ ğ‘£ çš„æ»‘åŠ¨å¹³å‡å€¼å¤§è‡´ç­‰äºè¿‡å» 1/(1âˆ’ğ›½) ä¸ªæ—¶åˆ» ğœƒ å€¼çš„å¹³å‡ï¼›å½“ ğ›½ è¶Šå¤§æ—¶ï¼Œæ»‘åŠ¨å¹³å‡å¾—åˆ°çš„å€¼è¶Šå’Œ ğœƒ çš„å†å²å€¼ç›¸å…³ã€‚å¦‚æœ ğ›½=0.9ï¼Œåˆ™å¤§è‡´ç­‰äºè¿‡å» 10 ä¸ª ğœƒ å€¼çš„å¹³å‡ï¼›å¦‚æœ ğ›½=0.99ï¼Œåˆ™å¤§è‡´ç­‰äºè¿‡å» 100 ä¸ª ğœƒ å€¼çš„å¹³å‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MNISTç›¸å…³çš„å¸¸æ•°\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# é…ç½®ç¥ç»ç½‘ç»œçš„å‚æ•°\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8 #åŸºç¡€å­¦ä¹ ç‡\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.001\n",
    "TRAINING_STEPS = 3000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼Œç»™å®šè¾“å…¥å’Œæ‰€æœ‰å‚æ•°ï¼Œè®¡ç®—å‰å‘ä¼ æ’­ç»“æœ\n",
    "# å®šä¹‰äº†ä¸€ä¸ªä½¿ç”¨RELUæ¿€æ´»å‡½æ•°çš„ä¸‰å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œé€šè¿‡åŠ å…¥éšè—å±‚å®ç°å¤šå±‚ç½‘ç»œç»“æ„\n",
    "# é€šè¿‡Reluæ¿€æ´»å‡½æ•°å®ç°å»çº¿æ€§åŒ–ï¼Œåœ¨å‡½æ•°ä¸­ä¹Ÿæ”¯æŒä¼ å…¥ç”¨äºè®¡ç®—å‚æ•°å¹³å‡å€¼çš„ç±»\n",
    "# æ–¹ä¾¿æµ‹è¯•æ—¶ä½¿ç”¨æ»‘åŠ¨å¹³å‡æ¨¡å‹\n",
    "\n",
    "def inference(input_tensor, avg_class, weights1, biases1,\n",
    "              weights2, biases2):\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,weights1) + biases1)\n",
    "        return tf.matmul(layer1,weights2) + biases2\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avgclass.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avgclass.average(biases2)\n",
    "\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_NODE], name='y-input')\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    y = inference(x, None, weights1,biases1,weights2,biases2)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # å¯¹æ‰€æœ‰èƒ½è®­ç»ƒçš„å˜é‡é‡‡å–æ»‘åŠ¨å¹³å‡æ“ä½œ\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # äº¤å‰ç†µå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè¿™é‡Œçš„äº¤å‰ç†µä½¿ç”¨sparse_softmax_cross_entropy_with_logitså‡½æ•°æ¥è®¡ç®—äº¤å‰ç†µ\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_,1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # è®¡ç®—L2æ­£åˆ™åŒ–æŸå¤±å‡½æ•°\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, \n",
    "                                               global_step, \n",
    "                                               mnist.train.num_examples/BATCH_SIZE,\n",
    "                                               LEARNING_RATE_DECAY)\n",
    "    \n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.group(train_step, variables_averages_op)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "with tf.Session() as sess:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
