{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist datasetæ˜¯æ‰‹å†™æ•°å­—çš„è®­ç»ƒæ•°æ®ï¼Œé¡¹ç›®çš„æœ¬è´¨ä¸Šæ˜¯è§£é‡Šå¦‚ä½•å°†å›¾ç‰‡åƒç´ æ•°æ®è½¬æ¢æˆå¯ä»¥åœ¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸­ä½¿ç”¨çš„æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 784)\n",
      "Y shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "xs, ys = mnist.train.next_batch(batch_size)\n",
    "print(\"X shape:\",xs.shape)\n",
    "print(\"Y shape:\",ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ·±åº¦å­¦ä¹ ä¸€æ–¹é¢éœ€è¦æ¿€æ´»å‡½æ•°å®ç°éçº¿æ€§åŒ–ï¼Œ\n",
    "### å¦å¤–ä¸€æ–¹é¢éœ€è¦ä½¿ç”¨ä¸€ä¸ªæˆ–è€…å¤šä¸ªéšè—å±‚ä½¿å¾—ç¥ç»ç½‘ç»œçš„ç»“æ„æ›´æ·±ï¼Œä»¥è§£å†³å¤æ‚é—®é¢˜\n",
    "\n",
    "> ä½†æ˜¯éšç€ç¥ç»ç½‘ç»œçš„ç»“æ„å˜å¾—å¤æ‚ï¼Œéœ€è¦å¸¦æŒ‡æ•°è¡°å‡çš„å­¦ä¹ ç‡è®¾ç½®ï¼Œä»¥ä¿è¯æ¢¯åº¦ä¸‹é™å®¹æ˜“æ”¶æ•›ï¼Œä½¿ç”¨æ­£åˆ™åŒ–æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨æ»‘åŠ¨å¹³å‡\n",
    "> æ¨¡å‹æ¥ä½¿å¾—æœ€ç»ˆçš„æ¨¡å‹æ›´åŠ å¥å£®\n",
    "> æ»‘åŠ¨å¹³å‡ğ‘¡ æ—¶åˆ»å˜é‡ ğ‘£ çš„æ»‘åŠ¨å¹³å‡å€¼å¤§è‡´ç­‰äºè¿‡å» 1/(1âˆ’ğ›½) ä¸ªæ—¶åˆ» ğœƒ å€¼çš„å¹³å‡ï¼›å½“ ğ›½ è¶Šå¤§æ—¶ï¼Œæ»‘åŠ¨å¹³å‡å¾—åˆ°çš„å€¼è¶Šå’Œ ğœƒ çš„å†å²å€¼ç›¸å…³ã€‚å¦‚æœ ğ›½=0.9ï¼Œåˆ™å¤§è‡´ç­‰äºè¿‡å» 10 ä¸ª ğœƒ å€¼çš„å¹³å‡ï¼›å¦‚æœ ğ›½=0.99ï¼Œåˆ™å¤§è‡´ç­‰äºè¿‡å» 100 ä¸ª ğœƒ å€¼çš„å¹³å‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MNISTç›¸å…³çš„å¸¸æ•°\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# é…ç½®ç¥ç»ç½‘ç»œçš„å‚æ•°\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8 #åŸºç¡€å­¦ä¹ ç‡\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.001\n",
    "TRAINING_STEPS = 3000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼Œç»™å®šè¾“å…¥å’Œæ‰€æœ‰å‚æ•°ï¼Œè®¡ç®—å‰å‘ä¼ æ’­ç»“æœ\n",
    "# å®šä¹‰äº†ä¸€ä¸ªä½¿ç”¨RELUæ¿€æ´»å‡½æ•°çš„ä¸‰å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œé€šè¿‡åŠ å…¥éšè—å±‚å®ç°å¤šå±‚ç½‘ç»œç»“æ„\n",
    "# é€šè¿‡Reluæ¿€æ´»å‡½æ•°å®ç°å»çº¿æ€§åŒ–ï¼Œåœ¨å‡½æ•°ä¸­ä¹Ÿæ”¯æŒä¼ å…¥ç”¨äºè®¡ç®—å‚æ•°å¹³å‡å€¼çš„ç±»\n",
    "# æ–¹ä¾¿æµ‹è¯•æ—¶ä½¿ç”¨æ»‘åŠ¨å¹³å‡æ¨¡å‹\n",
    "\n",
    "def inference(input_tensor, avg_class, weights1, biases1,\n",
    "              weights2, biases2):\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,weights1) + biases1)\n",
    "        return tf.matmul(layer1,weights2) + biases2\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avgclass.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avgclass.average(biases2)\n",
    "\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_NODE], name='y-input')\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    y = inference(x, None, weights1,biases1,weights2,biases2)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # å¯¹æ‰€æœ‰èƒ½è®­ç»ƒçš„å˜é‡é‡‡å–æ»‘åŠ¨å¹³å‡æ“ä½œ\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # äº¤å‰ç†µå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè¿™é‡Œçš„äº¤å‰ç†µä½¿ç”¨sparse_softmax_cross_entropy_with_logitså‡½æ•°æ¥è®¡ç®—äº¤å‰ç†µ\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_,1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # è®¡ç®—L2æ­£åˆ™åŒ–æŸå¤±å‡½æ•°\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, \n",
    "                                               global_step, \n",
    "                                               mnist.train.num_examples/BATCH_SIZE,\n",
    "                                               LEARNING_RATE_DECAY)\n",
    "    \n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.group(train_step, variables_averages_op)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer()\n",
    "        validation_feed = {x:mnist.validation.images, y:mnist.validation.labels}\n",
    "        test_feed = {x:mnist.test.images, y:mnist.test.labels}\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validation_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"validation_accuracy is %g\"%(validate_acc))\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x:xs,y_:ys})\n",
    "        test_acc = sess.run(accuracy,feed_dict=test_feed)\n",
    "        print(\"model is %g\"%(test_acc))\n",
    "\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "    train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avgclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5be245e2ed29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8066583c2344>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-8066583c2344>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(mnist)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mvariable_averages_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_averages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0maverage_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_averages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# äº¤å‰ç†µå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè¿™é‡Œçš„äº¤å‰ç†µä½¿ç”¨sparse_softmax_cross_entropy_with_logitså‡½æ•°æ¥è®¡ç®—äº¤å‰ç†µ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8066583c2344>\u001b[0m in \u001b[0;36minference\u001b[0;34m(input_tensor, avg_class, weights1, biases1, weights2, biases2)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mavgclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mavgclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avgclass' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[3.],\n",
      "       [4.],\n",
      "       [5.],\n",
      "       [6.]], dtype=float32)]\n",
      "use input_layer________________________________________\n",
      "[array([[3.],\n",
      "       [4.],\n",
      "       [5.],\n",
      "       [6.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.python.feature_column.feature_column import _LazyBuilder\n",
    "\n",
    "def test_numeric():\n",
    "    price = {'price': [[1.], [2.], [3.], [4.]]}  # 4è¡Œæ ·æœ¬\n",
    "    builder = _LazyBuilder(price)\n",
    "\n",
    "    def transform_fn(x):\n",
    "        return x + 2\n",
    "\n",
    "    price_column = feature_column.numeric_column('price', normalizer_fn=transform_fn)\n",
    "    price_transformed_tensor = price_column._get_dense_tensor(builder)\n",
    "    with tf.Session() as session:\n",
    "        print(session.run([price_transformed_tensor]))\n",
    "\n",
    "    # ä½¿ç”¨input_layer\n",
    "    price_transformed_tensor = feature_column.input_layer(price, [price_column])\n",
    "    with tf.Session() as session:\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([price_transformed_tensor]))\n",
    "\n",
    "test_numeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseTensorValue(indices=array([[0, 0],\n",
      "       [1, 0],\n",
      "       [3, 0]]), values=array([4, 1, 0]), dense_shape=array([4, 1]))]\n",
      "use input_layer________________________________________\n",
      "[array([[0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "def test_categorical_column_with_hash_bucket():\n",
    "#     color_data = {'color': [[2,3], [5,4], [-1,1], [0,1]]}  # 4è¡Œæ ·æœ¬\n",
    "    color_data = {'color': [[2], [5], [-1], [0]]}  # 4è¡Œæ ·æœ¬\n",
    "    builder = _LazyBuilder(color_data)\n",
    "    color_column = feature_column.categorical_column_with_hash_bucket('color', 5, dtype=tf.int32)\n",
    "    color_column_tensor = color_column._get_sparse_tensors(builder)\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        print(session.run([color_column_tensor.id_tensor]))\n",
    "\n",
    "    # å°†ç¨€ç–çš„è½¬æ¢æˆdenseï¼Œä¹Ÿå°±æ˜¯one-hotå½¢å¼ï¼Œåªæ˜¯multi-hot\n",
    "    color_column_identy = feature_column.indicator_column(color_column)\n",
    "    color_dense_tensor = feature_column.input_layer(color_data, [color_column_identy])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([color_dense_tensor]))\n",
    "\n",
    "test_categorical_column_with_hash_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
