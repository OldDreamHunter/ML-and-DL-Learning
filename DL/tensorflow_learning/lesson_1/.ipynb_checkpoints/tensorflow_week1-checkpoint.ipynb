{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四章 深层神经网络\n",
    "> 深度学习与深层神经网络\n",
    "\n",
    "> 损失函数定义\n",
    "\n",
    "> 神经网络优化算法\n",
    "\n",
    "> 神经网络进一步优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 深度学习与深层神经网络\n",
    "# 线性模型的局限性\n",
    "# 激活函数如何实现去线性化\n",
    "# 多层神经网络解决异或运算\n",
    "import tensorflow as tf\n",
    "a = tf.nn.relu(tf.matmul(x1,w1) + biases1)\n",
    "y = tf.nn.relu(tf.matmul(a,w2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 损失函数的定义\n",
    "# 经典的损失函数 回归和分类问题，会有不同的损失函数，机器学习模型分为三个部分，算法，损失函数，优化函数\n",
    "# 针对分类问题的损失函数一般是使用交叉熵，为什么要使用交叉熵呢，是为了计算两个概率分布之间的距离\n",
    "\n",
    "# 二分类问题的交叉熵\n",
    "cross_entropy = tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)+(1-y_)*tf.log(tf.clip_by_value(1-y,1e-10,1.0))))\n",
    "                               \n",
    "# 多分类问题的交叉熵\n",
    "cross_entropy = tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "\n",
    "                               \n",
    "# 交叉熵之后的结果一般会接入softmax，所谓softmax是根据这一次交叉熵的计算结果，然后再计算softmax，输出一个概率值\n",
    "# 一般配合使用                               \n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 自定义损失函数\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(v1,v2),a*(v1-v2), b*(v1-v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True  True]\n",
      "[2 3 4 4 5]\n"
     ]
    }
   ],
   "source": [
    "# 分析tf.where, tf.greater方法的应用，帮助自定义损失函数\n",
    "import tensorflow as tf\n",
    "v1 = tf.constant([1,2,3,4,5])\n",
    "v2 = tf.constant([2,3,4,0,0])\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "print(tf.greater(v1,v2).eval())\n",
    "\n",
    "print(tf.where(tf.greater(v1,v2),v1,v2).eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 自定义完损失函数之后，需要用优化算法来调用损失函数，从而达到优化参数的目的\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# 输入两个节点\n",
    "x = tf.placeholder(tf.float32, shape = (None,2), name = 'x_input')\n",
    "y_ = tf.placeholder(tf.float32, shape = (None,1), name = 'y_input')\n",
    "\n",
    "# 定义变量\n",
    "w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))\n",
    "y = tf.matmul(x,w1)\n",
    "\n",
    "# 定义损失函数\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "# loss = tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*loss_more,(y_-y)*loss_less))\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "train_step = tf.train.AdamOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# 通过随机数生成一个模拟函数\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size,2)\n",
    "Y = [[2*x1 + 1*x2] for (x1, x2) in X]\n",
    "\n",
    "# 训练神经网络\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 50000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "        if i % 10000 == 0:\n",
    "            print(sess.run(w1))\n",
    "#             print(sess.run(loss,feed_dict={x:X[start:end],y_:Y[start:end]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 神经网络的优化算法\n",
    "# 反向传播和梯度下降算法调整神经网络中的参数\n",
    "batch_size = n\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = (batch_size,2), name = 'x_input')\n",
    "y_ = tf.placeholder(tf.float32, shape = (batch_size,1), name = 'y_input')\n",
    "\n",
    "loss = ...\n",
    "train_step = tf.nn.AdamOptimizer().minimize(loss)\n",
    "\n",
    "#定义loss\n",
    "with tf.Session() as sess:\n",
    "    #定义迭代次数\n",
    "    #准备batch_size个训练数据，然后是训练数据随机打乱之后选取可以得到更好的优化效果\n",
    "    current_X, current_y = ...\n",
    "    sess.run(train_step, feed_dict = {x:current_X, y_:current_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 神经网络的进一步优化\n",
    "# 学习率（指数衰减的学习率）；过拟合问题（正则化）；滑动平均优化"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
