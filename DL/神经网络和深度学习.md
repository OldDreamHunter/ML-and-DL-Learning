# 第三周

### 浅层神经网络

* 输入层，隐藏层与输出层

* 激活函数的选择：

  a. 常见的激活函数有sigmoid; tanh; relu

  b. 神经网络需要非线性激活函数的原因就是增大了神经网络的模型容量，变换出更多的函数

  c. 激活函数的异同还有优缺点：

  sigmoid 和 tanh 其实是两个线性相关的激活函数；

  relu是为了解决sigmoid在训练过程中的梯度消失问题，同时不需要计算log函数，计算时间短，并且relu不存在梯度耗散，因此收敛快，同时大量的梯度为0，所以更新的时候只需要在意那些真正起作用的特征，但是同时它自己会出现神经元大量死亡的问题，因为有的时候w会因为梯度过大，导致更新变成负数，从而使得这个神经元死亡。针对这种问题有三种解决办法：1. leaky Relu; 2. 较小的学习率；3. momentum based优化算法

* 激活函数的导数
  
  * dg(z) = g(z)*(1-g(z))
  * dtanh(z) = 1-(tanh(z))^2
  
* 神经网络梯度下降

* 权重随机初始化（对权重w和偏置项b进行初始化）

  * 加快梯度下降速度
  * 避免影响准确性



### 深层神经网络

神经网络知识点：正向传播和反向传播，随机初始化权重，损失函数





