# 深度学习引言

## 第三周

### 浅层神经网络

* 输入层，隐藏层与输出层

* 激活函数的选择：

  a. 常见的激活函数有sigmoid; tanh; relu

  b. 神经网络需要非线性激活函数的原因就是增大了神经网络的模型容量，变换出更多的函数

  c. 激活函数的异同还有优缺点：

  sigmoid 和 tanh 其实是两个线性相关的激活函数；

  relu是为了解决sigmoid在训练过程中的梯度消失问题，同时不需要计算log函数，计算时间短，并且relu不存在梯度耗散，因此收敛快，同时大量的梯度为0，所以更新的时候只需要在意那些真正起作用的特征，但是同时它自己会出现神经元大量死亡的问题，因为有的时候w会因为梯度过大，导致更新变成负数，从而使得这个神经元死亡。针对这种问题有三种解决办法：1. leaky Relu; 2. 较小的学习率；3. momentum based优化算法

* 激活函数的导数
  
  * dg(z) = g(z)*(1-g(z))
  * dtanh(z) = 1-(tanh(z))^2
  
* 神经网络梯度下降

* 权重随机初始化（对权重w和偏置项b进行初始化）

  * 加快梯度下降速度
  * 避免影响准确性

## 第四周

### 深层神经网络

神经网络知识点：正向传播和反向传播，随机初始化权重，损失函数

***神经网络计算层数的时候只计算隐藏层和输出层，不包括输入层***

* 深度学习中的前向传播

  * 从单个样本x的前向传播讲到向量化的样本前向传播

* 为什么要深层表示

  * 神经网络的前几层是用来当作简单的函数，来探测边缘

    后面的几层是学习更多复杂的函数

* 搭建神经网络块

* 参数vs超参数

  * 学习率，梯度下降循环的数量，隐藏层数目，隐藏层单元数目，激活函数的选择，是他们控制了最后的参数w和b的值，所以他们称为超参数

  * 还有其他超参数，momentum,mini batch size,regularization

    parameters

  * 超参数只能尝试再比较，再选择最优的超参数，经常尝试，通过交叉验证等方式选择更好的超参数取值















