# 第二门课 改善深层神经网络：超参数调试，正则化，以及优化



## 第一周 深度学习的实践层面

* 训练，验证，测试集

训练的时候需要很多决策的内容，例如神经网络的层数，隐藏单元，学习速率是多少，以及各层分别采用哪些激活函数

深度学习已经应用在自然语言处理，计算机视觉，语音识别等众多领域，但是各个领域的超参数都有行业的不同，因为数据量等就不相同，需要多次循环往复，找到一个称心的神经网络。

训练集，验证集，测试集，分别根据数据来，一般训练集可以90%，验证集和测试集少一些。数据集规模较大的情况下，验证集和测试集的比例不超过20%或者10%

* 偏差与方差

训练集的误差/错误率称为偏差，测试集比训练集的误差/错误率的高低称为方差；高偏差就是欠拟合，高方差则是过拟合；

如何通过训练集的误差和验证集的误差来诊断算法是否存在高偏差和高方差，是否两个值都高，或者都不高。分析偏差和方差的原因是为了更系统地优化算法。

* 机器学习基础

如何分配训练集，测试集，还有验证集，以及它们各自的作用，为了有效地避免偏差和方差的情况，并且验证集是用来验证超参数的，当超参数变了之后，重新根据训练集训练好的模型，需要在验证集上来验证这个超参数是否比上一个超参数效果好。

* 正则化

深度学习很容易存在过拟合的问题--高方差，如何有效地避免高方差呢，有两个解决办法一个是正则化，一个就是准备更多的数据，训练集数据量少的时候，很容易造成训练集表现很好，但是在测试集上效果很差的情况。

为什么只正则化$w$而不需要正则化$b$呢，因为$w$是高维参数矢量，可以表达高方差的问题，并且在过拟合中起到关键原因的还是$w$。

$L2$正则化本质上是在$w$更新之前乘以了一个比1小的因子，导致其更新之后的权重会比原来不加正则化的时候要更新的快

* 正则化是如何防止过拟合的？

直观上说，$w$越复杂从而导致函数的运算越复杂，所以容易造成过拟合，因此在计算罗贝尼乌斯范数，或者$L2$范数的时候，如果把$λ$设置的足够的大，可能把$w$设置为接近于0的值，所以选择一个合适的λ值。

从$sigmoid$函数或者$tanh$函数中可以看出，如果$w$特别小，导致$z$计算出来很小，那么激活函数就没有起到非线性的作用，因此正则化$λ$越大，使得整个神经网络偏线性，所以可以缓解过拟合。

除了$L2$正则化以外，还有dropout正则化

* dropout正则化

dropout称为随机失活，工作原理是如果在训练神经网络，存在过拟合，那dropout会遍历网络的每一层，设置消除神经网络节点的概率，这样的话，会消除一些节点，然后删除从该节点进出的连线，最后得到一个规模更小的网络，然后用backprop去训练参数。

如何实施dropout呢？Inverted dropout，早期版本都没有除以keep-prob,

