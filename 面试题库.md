1、    针对机器学习中的分类任务，常用的模型评价指标是Accuracy、Precision、Recall、F1、AUC，请简述下各个评价指标的应用场景以及在场景中的优劣性？

* Accuracy

  Accuracy是最基本的评价指标，但是针对二分类并且正负样例不平衡，并且我们重点评价数量小的类别，Accuracy就没有参考价值，例如癌症检测，欺诈检测等。

* Recall，Precision，F1-Score

  Recall是测试集中正例被模型预测结果覆盖的情况，Precision是从模型预测为正的结果中，真实也为正的比例。只评价我们感兴趣的类别的Recall和Precision（例如label等于1）。

  如果模型很贪婪，希望覆盖的样本多，那么模型的Recall很高，但是Precision会很低；

  如果模型很保守，只对它确定的样本作出预测，那么模型的Precision很高，但是模型的Recall就会很低；

  F1是综合考虑Precision和Recall的指标，用二者的调和平均。1/F1score = 1/2*(1/recall + 1/precision)

* AUC（Area Under Curve）

  在分类问题的评价中，通常有两个常用的Curve，一个是PR曲线（Precision-Recall），一个是ROC曲线（receiver-operating-characteristic Curve **接收者操作特征曲线**），我们常说的AUC就是Area Under ROC curve。ROC以TPR和FPR为轴，根据不同的threshold的值画的曲线。

  TPR = TP/(TP+FN)，预测为正的样本中，预测正确的比例，就是召回率

  FPR = FP/(FP+TN)，预测为负的样本中，预测错误的比例

  

2、    请问你在机器学习的项目中是否遇到过overfitting的问题？你是如何解决上述情况的？理论上是否还存在其他的方法可以解决上述问题？

* overfitting即模型在训练集上表现好，但在测试集上表现不好

* 解决方法：

  * 增加训练数据； 提前终止；正则化；
  * 借鉴Random Forest中bootstrap采样之后训练多个不同的模型，对模型进行融合；
  * 深度学习中还有dropout和数据增强；

  

3、    在机器学习的分类任务中，大部分分类任务都存在训练样本不均衡的问题，请问有哪些方法可以解决训练样本不均衡的问题？这些方法各有什么优缺点？

* 采样。过采样（oversample） 和 降采样（undersample），oversample是对样本少的类的数据随机多采样（或者加随机噪声采样），undersample是对样本多的类随机少采样。过采样容易造成过拟合（SMOTE算法可以避免），降采样又有可能丢失重要信息（Informed Undersampling）。
* 设置权重。对于样本少的类设置更高的权重
* 评价方法。用AUC或者F1-score进行评价

 

4、    在机器学习中，特征工程部分对项目起着至关重要的作用，如何分析判断特征对于模型的有效性？举例说明。

* feature importance，例如训练一个模型输出各特征的feature importance
* wrapper，例如去除某个特征之后，看剩余特征训练的模型表现
* 和特征相关性，分析特征和标签之间的相关系数

三种方法综合使用，把结果融合起来考虑



5、    讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系

* GBDT的算法细节和目标函数

  GBDT的主要思想是构造一组弱分类器，每个弱分类学习目标是上一个弱分类器的残差，最终将多个弱分类器的结果加起来

  GBDT主要是三个方面，GB，DT，Shrinkage，其中DT是用的CART树中的回归树，GB就是损失函数在函数空间的梯度(残差)，shrinkage类似于神经网络中的学习率，在梯度方向上下降的慢一些，更容易收敛

  GBDT目标函数，如果是MSE，则损失函数的梯度正好是残差

* 与adaboost的区别和联系

  联系：都是boost方法，下一次的弱分类器的任务都与上一次的有关，并且都是加法模型

  区别：主要是boost的方式，adaboost的思想是把本轮分错的样本的权重提高，在下一轮学习的时候重点学习上一个分类器分错的样本。涉及到两个权重的更新，一个是样本权重的更新，一个是分类器权重的更新。GBDT的boost是不断学习上一个学习器的残差
  
  

6、 XGBoost作为经典的集成树模型，它是如何寻找最佳分割点？对于缺失值，它是如何如何处理缺失值的？

* 利用分位点算法查找最佳分割点，

 

7、    LightGBM作为XGBoost的改进版本，它们有什么区别？

* xgboost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略，区别是xgboost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是xgboost也进行了分裂，带来了务必要的开销。 leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。 

* lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。 

* 直方图做差加速 ，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。 

* lightgbm支持直接输入categorical 的feature，在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。 

  

8、    特征选择在机器学习起着非常重要的作用，常用的特征选择有哪几种方法？各有什么优缺点？

* filter

  * 优点：通用性强，适合大规模的数据，可以除去大量的不相关特征，作为算法的预筛选器合适
  * 缺点：由于算法的评价标准独立于其他算法，因此准确性会低于wrapper的筛选结果

* wrapper

  * 优点：相对于filter方法，筛选的特征分类性能更好
  * 缺点：通用性不强，只能用最终算法的重新进行特征选择，每次对子集评价都需要重新训练和评估，算法复杂度比较高

* embedding

  * 嵌入到算法里面的特征选择，包括正则化，决策树

    

9、    基于每日用户搜索内容，假设只有少量已知商品的情况下，如何根据用户搜索内容获取平台内没有的新商品？

 

10、  为什么logistic回归的要用sigmoid函数？优缺点？

* 为什么用logistic回归？在建模预测 Y|X，并认为 Y|X 服从bernoulli distribution，所以只需要知道 P(Y|X)；其次需要一个线性模型，所以 P(Y|X) = f(wx)，可以通过最大熵原则推出的这个 f，就是sigmoid。

* 优点：把数据压缩到0到1之间
* 缺点：1. 容易梯度消失，x稍大的情况就会趋近于一条线；2. 非0均值，结果容易全正全负



11、  常见的正则化有什么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？其他防止过拟合的方法有？（和题目2类似）

* 常见的正则化有：L1, L2正则化；
* 正则化作用是为了防止过拟合，其中L1会使得权重稀疏，而L2会使得权重衰减；
* 其他防过拟合的方法：增加训练数据，早停，dropout等。



12、  分类模型如何选择？如何判断效果？如何计算AUC？你最熟悉的集成分类模型是什么？请简述其原理？

* 常见的分类模型，lr，svm，naive bayes, dt, rf, gbdt, xgb, lightgbm，如果数据集很大，一般是从易用性和运行速度角度来选择，但是如果很在意正确率，则需要尝试多个分类模型，根据在测试集上的评价指标来选择最终的模型。

* 常见的评价指标，准确率（accuracy），召回率（recall），精确率（precision）,  F$β$-Score，PRCurve,  真正率（True Positive Rate），假正率（False Positive Rate），ROC（Receiver Operating Characteristic），AUC（Area Under Curve）

* AUC的计算

  在有M个正样本,N个负样本的数据集里。一共有MN对样本（一对样本即，一个正样本与一个负样本）。统计这MN对样本里，正样本的预测概率大于负样本的预测概率的个数。

*  集成分类模型：random forest, adaboost, gbdt
  * bagging: rf, 思想是bootstrap + 投票打分
  * boosting: 每棵树的学习目标与上一棵树的结果有关。adaboost的重点是两个权重，一是数据的权重，二是树的权重，学错的数据样本权重提高（提高多少与这轮学习的树的权重有关），在下一棵树中重点去学，树的权重与它的准确性有关，准确性低的树权重小，准确性高的树权重高。gbdt是gb + dt，用回归树学习目标，每一棵树的学习目标是上一棵树学习结果的残差。残差为什么又是梯度呢，对于mse的损失函数而言，损失函数的一阶导数正好是残差，用梯度来拟合不同损失函数的残差。



13、  SVM中用到了哪些核？SVM中的优化技术有哪些？

* 核函数的选择，线性核函数，多项式核函数，高斯核函数，sigmoid核函数
* SMO优化算法

 

14、  SVM如何学习超平面？用数学方法详细解释一下。

 

15、  讲一下PCA的步骤。PCA和SVD的区别和联系

PCA的本质是选择一个平面，使得数据点投影到这个平面的时候包含的信息量最大。可以从最大化方差和最小化投影误差两个角度解释。

 

16、  介绍一下无监督学习，算法有哪些？

* 无监督学习指的是把相似的物品聚在一起，不关心是哪一类。
* 聚类算法有KMeans，DBSCAN，层次聚类法；
* 关联算法有Apriori，FP-growth



17、  如何部署机器学习模型？

* python flusk + gunicorn + nginx
* pmml + java
* tensorflow + tensorflow serving + spark streaming



18、  简述word2vec的原理

* 传统的one-hot表示的词向量很稀疏，而且表示的词汇量有限，因此需要通过训练得到词的分布式的表示，将每个词映射到较短的词向量上，这样所有的词向量就构成了向量空间。
* word2vec通过两种结构构造神经网络，一种是cbow，而另外一种是skip-gram，两种改进思路，hiearchical softmax 和 negative sampling。



19、  word2vec为什么 不用现成的DNN模型，要继续优化出新方法呢？有哪些方法可以优化word2vec训练时间？

* 因为DNN模型处理非常耗时，并且如果词汇表特别大，softmax对每个词输出概率的计算量就很大。
* 霍夫曼树，negative sampling 和 hiearchical softmax

 

20、  word2vec和glove相比有什么优缺点？

原来将每个词利用one hot，做成词向量，但是稀疏性特别大，并且词汇集越大，每个词的维度就越多，因此会造成维度灾难。并且词向量可以表示成密集向量，用较少的维度表示一个词，这种称为word embedding。而word2vec和glove是两种做embedding的方法。

* word2vec有两种模型结构，cbow和skip-gram，其中cbow是根据目标单词的上下文经过映射来预测目标单词，但是skip-gram是根据目标单词经过映射预测上下文（两者的区别可以类比一对多教学和多对一教学，cbow就是一群学生面对一个老师，skip-gram是一群老师教一个学生）
  * 基于hierachical softmax的cbow和skip-gram，全是正例的训练
  * 基于negative sampling的cbow和skip-gram，部分负例，使得模型学习到的word_embedding更全面
* 在skip-gram的基础上加入单词的统计信息。

缺点：word2vec是局部语料训练，特征提取是滑窗

glove是为了构建occurance matrix，基于全局语料

 

21、  LSTM中每个 gate 的作用是什么，为什么跟 RNN 比起来，LSTM 可以防止梯度消失？

* 每个gate的作用：
  * 忘记门：将之前的信息选择性的遗忘；
  * 输入层门：将新的信息选择性地记录；
  * 输出层门：确定输出；
* **RNN梯度消失的原因，LSTM如何可以防止梯度消失**
  * 不太理解



22、  pooling 的作用是什么，为什么 max pooling 更常用？什么情况下 average pooling 更合适？

* 位移不变性；减少计算量；提升感受野的大小；
* 通常max pooling采样效果更好，max pooling能提取出辨识度更好的特征
* average pooling通常在最后一层或者是末端，代替flatten，使得输入数据变成一维向量

 

23、  梯度消失和梯度爆炸的原因是什么，有哪些解决方法？

* 梯度消失的原因一般是选择了不合适的损失函数，例如sigmoid；
* 梯度爆炸的原因一般是初始化权值太大的时候；
* 解决办法是：1.  梯度剪切（限制梯度的范围）和正则化（有效控制梯度爆炸）；2. 改变激活函数(relu，leaky relu)；3. batch normalization

 

24、  CNN 和 RNN 的梯度消失是一样的吗？

* CNN和RNN的梯度消失都是由于层数过深导致，CNN网络层数过深，RNN是时间迭代计算太长

 

25、  CNN、RNN以及transformer在提取特征时的优缺点？

* CNN
  * 优点：可以并行
  * 缺点：无法捕捉长距离的特征，因此提出了两种解决办法，一个是Dilated CNN，间隔卷积；一个是深层CNN，靠深度捕捉特征；输入定长，不够的用padding补充
* RNN
  * 优点：捕捉长距离的特征，可以接受不定长的输入
  * 缺点：大规模并行的问题，改进是用隐藏层之间并行

26、  介绍 sigmoid，tanh，relu 各自的有点和适用场景

为什么要使用非线性激活函数？如果不使用激活函数，这种情况下每一层输出都是上一层输入的线性函数。无论神经网络有多少层，输出都是输入的线性函数，这样就和只有一个隐藏层的效果是一样的。

* tanh
  * 优点：零均值；压缩数据；幂运算耗时
  * 缺点：梯度消失；幂运算耗时
* sigmoid
  * 优点：便于求导；能压缩数据，保证数据幅度不会又问题；适合于前向传播；
  * 缺点：容易出现梯度消失的现象；非0均值导致后续的前向传播会出现偏向；幂运算耗时
* relu
  * 优点：收敛更快，解决梯度消失问题；计算复杂度低；适合反向传播
  * 缺点：输出不是0均值；神经元坏死现象 

27、  relu 的负半轴导数都是 0，这部分产生的梯度消失怎么办？

* relu如果中间某层由于学习率或者导数过大，导致更新w之后，前向传播结果进入了负半区，那这个神经元会出现死亡的现象。
* 改进：LReLu, PReLu, ELU等，其中LReLu/PReLu都是在负半区也设置一个对应的激活函数$αx$，其中LReLu是固定的$α$值，一般取数比较小，PReLu能动态调整负半轴斜率$α$值

 

28、  batch size 对收敛速度的影响

* batch size小，收敛变慢，不容易收敛，极端batch size=1，就是普通的随机梯度下降

* batch size增大，充分利用内存，学习速率变快，收敛速度会加快

* batch size再增大，极端情况batch size=数据集大小，则导致需要学习多个epoch才能达到好的效果

  反而降低了收敛速度



29、  **介绍 batch normalization**

* 因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。

 

30、  如何理解 Dropout？分别从 bagging 和正则化的角度

* Dropout的工作流程：

  * 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变；
  * 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数(w,b)；
  * 然后继续重复这一过程:
    - 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）
    - 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）
    - 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）
  * 预测的时候需要给每个神经元的输出乘以概率P

* 从bagging和正则化角度

  bagging是boostrap之后训练多个模型，再把模型预测结果取平均；

  正则化包括L1和L2正则化，L1正则化使得w变稀疏，L2正则化使得w变小；

  * **（1）取平均的作用：** 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。
  * **（2）减少神经元之间复杂的共适应关系：** 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。

 

31、  介绍各种优化方法，如 sgd，momentum，rmsprop，adam

* SGD: 每读入一条数据，就计算一次梯度，并更新权重$Θ=Θ−α⋅▽ΘJ(Θ;x(i),y(i))$

  Batch Gradient Descent; Mini-Batch Gradient Descent

  问题都是很难选择一个合理的学习率$α$

* momentum：$vt=γ⋅vt−1+α⋅▽ΘJ(Θ)$             $Θ=Θ−vt\Theta = \Theta-v_{t}$                  $Θ=Θ−vt$

  若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减

* rmsprop: rmsprop是adagrad的改进版，adagrad会对每个单独的权重$ Θi$改变其学习率，但是分母是历史梯度的平方和，容易造成衰减过快，rmsprop用的是历史梯度的均值，并且对Θ进行更新，不用分别用不同的学习率更新$ Θi$

* Adam：是momentum和rmsprop的结合，一方面梯度用滑动平均替代，另一方面学习率除以历史梯度的平方和，减缓学习率



32、  如果训练的神经网络不收敛，可能有哪些原因？

* 数据没有归一化

* 训练数据太大

* 学习率和优化函数选择

* 输出层选错激活函数（分类问题和回归问题区别）

* 神经元大量死亡，通常用Relu会遇到，改为leaky Relu

* 初始化权重错误

  

33、  如何理解卷积核，1*1 的卷积核有什么用？

* 卷积核的作用是提取图像更高维的特征

* 1*1卷积核不改变图像的宽高，只改变图像的维度（例如图像中的通道数）